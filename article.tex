%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{lscape}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{hyperref}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Journal}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Biased Random Key Genetic Algorithm for the p-dispersion problem}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[PUC-Rio]{Lucas Ximenes Guilhon}

\affiliation[inst1]{organization={Department of Industrial Engineering},%Department and Organization
            addressline={Rua Marquês de São Vicente, 225}, 
            city={Rio de Janeiro},
            postcode={22451-900}, 
            state={Rio de Janeiro},
            country={Brazil}}


\begin{abstract}

The p-dispersion problem, a variant of the maximum dispersion problem, presents a challenging computational task with applications in various domains, notably facility location and optimization. This NP-hard problem involves selecting a subset of $p$ objects from a set of $n$, aiming to maximize the minimum dissimilarity between objects within the chosen subset. Despite years of research, the success of exact methods has hindered the exploration of heuristic approaches, leaving specific instance types under addressed. In this study, we propose a biased random key genetic algorithm (BRKGA) to tackle the p-dispersion problem and conduct a comparative analysis against established state-of-the-art methods. Computational analysis was performed using instances sourced from MDPLIB 2.0, encompassing Euclidean, Real, and Integer datasets. The results shed light on the performance of the BRKGA method in addressing this challenging problem and offer insights for its broader applications.\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% \includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
BRKGA \sep p-dispersion
%% PACS codes here, in the form: \PACS code \sep code
% \PACS 0000 \sep 1111
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
% \MSC 0000 \sep 1111
\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:intro}

The p-dispersion problem, also known as the max-min or maxmin-min version of the maximum dispersion problem, is a combinatorial optimization problem that can be described as follows:

Given a set of $n$ objects and an $n \times n$ dissimilarity matrix, the goal is to identify a subset of $p$ objects in such a way that it maximizes the minimum dissimilarity between any pair of objects within this subset. This problem is classified as NP-hard \cite{Erkut1990-mo}, with diverse applications across various domains. One of the most prevalent applications is in facility location, where the objects represent facilities, and the dissimilarity values denote the distances between them. Ensuring maximum dispersion of facilities is critical for various reasons, including minimizing the impact of a single event on all facilities and reducing competition or overlap between facilities.

The p-dispersion problem's versatility allows it to be adapted to specific scenarios, like selecting the most dispersed $p$ points from a Pareto front in a multi-objective optimization problem to ensure a well-rounded coverage of different preferences \cite{Dupin2023-vx}. It can also be applied to choosing the most diverse $p$ products in a portfolio to maximize the diversity of a product portfolio \cite{Erkut1990-mo}.

Extensive research has been dedicated to the p-dispersion problem, resulting in a variety of solution methods. Unlike other maximum diversity problems, exact methods have demonstrated significant success in solving the p-dispersion problem, leading to relatively fewer studies on heuristic approaches. Nevertheless, current exact methods still face limitations in handling specific instance types, such as those with randomized dissimilarity matrices or scenarios involving a large number of facilities to be chosen. In this paper, we introduce a biased random key genetic algorithm (BRKGA) \cite{Goncalves2011-kp} as an innovative approach to address the p-dispersion problem. We also provide a comparative analysis of its performance against existing state-of-the-art methods.

%%       \citet{<label>} ==> Jones et al. [21]
%%       \citep{<label>} ==> [21]
%%

\section{Related work and problem formulation}
The Max-Min version of the maximum dispersion problem (MDP), commonly known as the p-dispersion problem, was initially introduced by \citet{Moon1984-zi}. However, it was \citet{Kuby1988-ii} who pioneered the development of a mixed-integer linear programming formulation for solving this discrete optimization problem. Subsequently, \citet{Erkut1990-mo} enhanced this formulation and proposed a branch-and-bound algorithm for its solution, represented by Equation \ref{eq:erkut}. In this equation, $N$ denotes the set of objects, $x_i$ is a binary variable indicating the selection of object $i$, $d_{ij}$ signifies the dissimilarity between objects $i$ and $j$, and $M$ is a substantial positive constant.

\begin{equation}
    \label{eq:erkut}
    \begin{aligned}
        \max \quad & z \\
        \text{s.t.} \quad & z \leq M(2 - x_i - x_j) + d_{ij} \quad \forall i,j \in N \ | \ i \leq j \\
        & \sum_{i \in N} x_i = p \\
        & x_i \in \{0,1\} \quad \forall i \in N
    \end{aligned}
\end{equation}

Following initial efforts to solve the p-dispersion problem exactly, the landscape shifted towards heuristic methods. Techniques like simulated annealing and tabu search were explored by \citet{Kincaid1992-mp}, while \citet{Ghosh1996-zm} and \citet{Resende2010-vr} introduced the GRASP heuristic. For an in-depth review of these methods, a comprehensive survey by \citet{Marti2022-ku} provides an extensive overview of the history of maximum diversity problems and their corresponding solution strategies.

However, a significant breakthrough occurred with the development of more efficient integer programming formulations in the 2010s. Notably,\citet{franco2015solving} introduced a set packing-based approach to establish upper and lower bounds for the objective function. Equation \ref{eq:franco} presents this formulation, which allows for a binary search within the dissimilarity matrix to find the optimal value of $r$. This enhanced formulation vastly expanded the solvability of larger instances, accommodating up to 1000 objects, compared to the original formulation's limitation of 100 objects.

\begin{equation}
    \label{eq:franco}
    \begin{aligned}
        \max \quad & 0 \\
        \text{s.t.} \quad & \sum_{i \in N} x_i = p \\
        & x_i + x_j \leq 1 \quad i,j \in N \ |\  d_{ij} \leq r \\
        & x_i \in \{0,1\} \quad \forall i \in N
    \end{aligned}
\end{equation}

The current state-of-the-art method for the p-dispersion problem, proposed by \citet{Contardo2020-vn}, employs a clustering technique to decompose the problem into a set of smaller p-dispersion problems. This method excels at solving instances with a high number of objects, reaching up to 100,000 objects, a feat unattainable by previous approaches. Nonetheless, it exhibits limited scalability for larger values of $p$.

It's important to note that previous research on the p-dispersion problem lacked a standardized instance library, making it challenging to compare the performance of different methods. To address this, we adopted MDPLIB 2.0 \citep{marti2021mdplib}, a comprehensive library designed for maximum diversity problems, in our benchmarking efforts.

In this study, we propose the utilization of a biased random key genetic algorithm (BRKGA) \citep{Goncalves2011-kp} to address the p-dispersion problem and offer a comparative assessment against existing state-of-the-art methods. Our objective is twofold: to tackle instances that elude other approaches and to establish a benchmark for future methods to evaluate their performance against.
\section{Solving the p-dispersion problem with a BRKGA}
\label{sec:brkga}
The Biased Random Key Genetic Algorithm (BRKGA) is a powerful and diverse metaheuristic used to solve complex combinatorial problems. One of it's main benefits is the fact that it simplifies one of the biggest challanges in genetic algorithms, which is how to perform crossover on a population of solutions without creating infeasibilities.
This simplification is made possible by the use of a decoder, which is responsible for transforming the chromosome, i.e a vector of real numbers in the range of $[0,1]$, into a feasible solution. Now the act of crossing solutions simply involves choosing indices from the chromosomes of the parents and combining them into the chromosome of the child. This process is illustrated in Figure \ref{fig:crossing}.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{crossing.png}
    \caption{Crossover of two parents (solutions $a$ and $b$) into a new child (solution $c$). \href{https://mauricio.resende.info/talks/2012-09-CLAIO2012-brkga-tutorial-both-days.pdf}{Source}}
    \label{fig:crossing}
\end{figure}
This decoder is of course problem dependent, and in the case of the p-dispersion problem, it is responsible for selecting the $p$ objects with which the solution will be composed, and we'll discuss it in more detail in the following section.
\subsection{Chromosome representation and decoder}
As mentioned before, the BRKGA represents solutions as chromosomes, which are vectors of real numbers in the range of $[0,1]$. And in order for this representation to be useful, a decoder is required to transform the chromosome into a feasible solution. In the case of the p-dispersion problem, the decoder is responsible for selecting the $p$ objects with which the solution will be composed.
In our case, this is done by sorting the chromosome and setting the $p$'s equal to the indices of the $p$ smallest values in the chromosome.
This process is ilustrated in the pseudocode below:
\begin{algorithm}[H]
    \label{alg:decoder}
    \begin{algorithmic}[1]
        \Require $C \in [0, 1]^M$
        \State $S_i \leftarrow \text{argsort}(C)$
        \State $P \leftarrow S_i[1:p]$
        \State \Return $min \{ d(p_i, p_j) \ | \ p_i, p_j \in P \}$
    \end{algorithmic}
    \caption{Decoder for the p-dispersion problem}
\end{algorithm}
In which $C$ is the chromosome, $S_i$ are the indices of the sorted chromosome, $P$ is the set of the chosen objects, and $d(p_i, p_j)$ is the dissimilarity between objects $p_i$ and $p_j$.

\subsection{Local search}
Two neighborhoods were explored in this study and both of them were swap neighborhoods. The firt one, called \textit{Fast Swap}, uses a preprocessed $n\times p$ matrix that contains the closest $p$ points to every point in the instance. This matrix is calculated once and used in every iteration of the local search. The way it works is that it finds the pair in the solution that contains the smallest distance, and then it attemps to swap one of the points in the pair by one of it's $p$ closest points. If the swap improves the solution, it is accepted.
The second neighborhood, called \textit{Swap}, is a more traditional swap neighborhood, where we evaluate the swap of every point $p$ in the solution with every other point $n$ that is not in the solution. If the swap improves the solution, it is accepted.
The pseudocode for both neighborhoods is presented below:
\begin{algorithm}[H]
    \label{alg:swap}
    \begin{algorithmic}[1]
        \Require $P$ \text{the set of chosen points}
        \For{$i \in 1..N$}
            \For{$j \in 1..|P|$}
                \State $\text{eval}(P[j], i)$ \Comment{Evaluate the swap of $P[j]$ with object $i$}
            \EndFor
        \EndFor
    \end{algorithmic}
    \caption{Swap neighborhood}
\end{algorithm}

\begin{algorithm}[H]
    \label{alg:fastswap}
    \begin{algorithmic}[1]
        \Require $P,\ M$ \text{the set of chosen points and the matrix of closest points}
        \State $(i, j) \leftarrow \text{argmin}_{i,j \in P} d(i, j)$ \Comment{Find the pair of points with the smallest distance}
        \State $c_1 \leftarrow \text{eval}(P[i], M)$ \Comment{Evaluate the swap of $P[i]$ with it's $p$ closest points}
        \State $c_2 \leftarrow \text{eval}(P[j], M)$ \Comment{Evaluate the swap of $P[j]$ with it's $p$ closest points}
        \If{$c_1 < c_2$}
            \State $P[i] \leftarrow \text{argmin}_{k \in M[i]} d(k, P[j])$ \Comment{Swap $P[i]$ with the closest point to $P[j]$}
        \Else
            \State $P[j] \leftarrow \text{argmin}_{k \in M[j]} d(k, P[i])$ \Comment{Swap $P[j]$ with the closest point to $P[i]$}
        \EndIf
    \end{algorithmic}
    \caption{Fast Swap neighborhood}
\end{algorithm}
The evaluate function is responsible for calculating the cost of the solution after the swap, and it does so by checking the smallest distance between every pair of points, amounting to a complexity of $O(p^2)$.
\subsection{Evolutionary process}
The evolutionary process in our BRKGA implementation involves multiple stages. Initially, an initial population of solutions, represented as chromosomes, is generated randomly. These solutions evolve iteratively through a series of genetic operations like crossover and mutation.

During each iteration of the evolution loop, the top five and bottom five solutions from the current population are selected based on their fitness. A local search procedure is then applied exclusively to these ten solutions. This local search aims to refine the solutions further by exploring their neighborhood for potential improvements. Importantly, this local search is not applied to the entirety of the population, but only to these ten solutions. After local search, the evolutionary process continues by performing genetic operations on the entire population, iterating towards potentially better solutions. This iterative process continues until a termination criterion is met or a specified number of generations is reached.

\section{Experimental Results}
\subsection{Computational Environment}
The computational experiments were conducted on a computer with an AMD Ryzen 7 6800U processor operating at 2.70 GHz, equipped with 16 GB of RAM, and running the Windows 11 operating system. The exact algorithm was implemented using Julia 1.8.5, with modeling facilitated by JuMP \cite{Lubin2023} and mathematical model solving performed with Gurobi 10.0.2. We adopted the Julia version of the BRKGA-MP-IPR framework \cite{Andrade2021-pe} as our chosen BRKGA implementation.

\subsection{Instances}
The instances used in this study were sourced from MDPLIB 2.0 \cite{marti2021mdplib}, a comprehensive library of instances designed for maximum diversity problems, containing a total of 770 instances. For the purposes of this paper, we focused exclusively on instances addressing the non-constrained version of the maximum diversity problem. These instances are categorized into three sets: Euclidean, Real, and Integer instances.

The Euclidean dataset comprises 160 matrices, with values calculated as the Euclidean distances between randomly generated points, each having coordinates within the range of 0 to 10. Meanwhile, the Real and Integer sets consist of 140 and 170 instances, respectively. These sets feature matrices with numbers generated from uniform distributions of real and integer numbers. Each instance subset is further divided into subcategories, with more details available in the original paper \cite{marti2021mdplib}.
\subsection{Results}

\begin{landscape}
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
        ~ & Exact & ~ & ~ & BRKGA & ~ & Diff & ~ \\ \hline
        Instance & Result & Time (s) & Optimal & Result & Time (s) & Result & Time (s) \\ \hline
        GKD-c\_1\_n500\_m50.txt & 10.62826 & 175.0819419 & FALSE & 10.0025 & 120.0700002 & 5.89\% & 31.42\% \\ \hline
        GKD-c\_2\_n500\_m50.txt & 10.69907 & 204.1420088 & FALSE & 9.94636 & 123.2719998 & 7.04\% & 39.61\% \\ \hline
        GKD\_d\_1\_n1000\_coor.txt & 9.58187 & 59.3382452 & TRUE & 6.90829 & 154.8179998 & 27.90\% & -160.91\% \\ \hline
        GKD\_d\_1\_n100\_coor.txt & 34.11047 & 0.131707 & TRUE & 33.00549 & 3.313999891 & 3.24\% & -2416.19\% \\ \hline
        GKD\_d\_1\_n2000\_coor.txt & 6.50899 & 176.5808659 & FALSE & 4.55931 & 655.9489999 & 29.95\% & -271.47\% \\ \hline
        GKD\_d\_1\_n250\_coor.txt & 20.34579 & 0.8918694 & TRUE & 17.36325 & 13.16700006 & 14.66\% & -1376.34\% \\ \hline
        GKD\_d\_1\_n25\_coor.txt & 80.28497 & 0.0337449 & TRUE & 80.28497 & 0.9089999199 & 0.00\% & -2593.74\% \\ \hline
        GKD\_d\_1\_n500\_coor.txt & 13.71912 & 6.4232128 & TRUE & 11.82253 & 93.81500006 & 13.82\% & -1360.56\% \\ \hline
        GKD\_d\_1\_n50\_coor.txt & 54.21418 & 0.0661512 & TRUE & 54.21418 & 1.605999947 & 0.00\% & -2327.77\% \\ \hline
        GKD\_d\_2\_n1000\_coor.txt & 9.50384 & 55.7057781 & TRUE & 6.70564 & 126.836 & 29.44\% & -127.69\% \\ \hline
        GKD\_d\_2\_n100\_coor.txt & 34.81992 & 0.1381636 & TRUE & 30.27803 & 3.405999899 & 13.04\% & -2365.19\% \\ \hline
        GKD\_d\_2\_n2000\_coor.txt & 6.5635 & 200.8414534 & FALSE & 4.49922 & 1080.0041 & 31.45\% & -437.74\% \\ \hline
        GKD\_d\_2\_n250\_coor.txt & 20.54752 & 1.1130759 & TRUE & 17.27553 & 14.61300015 & 15.92\% & -1212.85\% \\ \hline
        GKD\_d\_2\_n25\_coor.txt & 78.42063 & 0.066236 & TRUE & 78.42063 & 0.9419999123 & 0.00\% & -1322.19\% \\ \hline
        GKD\_d\_2\_n500\_coor.txt & 13.93588 & 6.3882725 & TRUE & 12.52089 & 120.023 & 10.15\% & -1778.80\% \\ \hline
        GKD\_d\_2\_n50\_coor.txt & 54.20737 & 0.0985436 & TRUE & 54.20737 & 1.993999958 & 0.00\% & -1923.47\% \\ \hline
    \end{tabular}
\end{table}
\end{landscape}
\begin{landscape}
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
    ~ & Exact & ~ & ~ & BRKGA & ~ & Diff & ~ \\ \hline
        MDG-a\_1\_100\_m10.txt & 4.68 & 5.3068942 & TRUE & 3.86 & 5.032000065 & 17.52\% & 5.18\% \\ \hline
        MDG-a\_1\_n500\_m50.txt & 0 & 160.6789812 & FALSE & 0.5 & 124.4679999 & \#DIV/0! & 22.54\% \\ \hline
        MDG-a\_2\_100\_m10.txt & 4.74 & 2.9565136 & TRUE & 3.52 & 3.467000008 & 25.74\% & -17.27\% \\ \hline
        MDG-a\_2\_n500\_m50.txt & 0 & 163.7505464 & FALSE & 0.57 & 90.62800002 & \#DIV/0! & 44.65\% \\ \hline
        MDG-b\_1\_100\_m10.txt & 460.11 & 3.4369108 & TRUE & 417.98 & 3.493999958 & 9.16\% & -1.66\% \\ \hline
        MDG-b\_1\_n500\_m50.txt & 0 & 151.1147347 & FALSE & 72.83 & 120.096 & \#DIV/0! & 20.53\% \\ \hline
        MDG-c\_1\_n3000\_m300.txt & 0 & 131.233303 & FALSE & 5 & 4800 & \#DIV/0! & -3557.61\% \\ \hline
        MDG-c\_2\_n3000\_m300.txt & 0 & 131.695023 & FALSE & 5 & 4800 & \#DIV/0! & -3544.78\% \\ \hline
        SOM-a\_11\_n50\_m5.txt & 7 & 0.174539 & TRUE & 6 & 1.858999968 & 14.29\% & -965.09\% \\ \hline
        SOM-a\_1\_n25\_m2.txt & 9 & 0.031464 & TRUE & 9 & 1.003000021 & 0.00\% & -3087.77\% \\ \hline
        SOM-a\_21\_n100\_m10.txt & 5 & 3.5217837 & TRUE & 2 & 3.919999838 & 60.00\% & -11.31\% \\ \hline
        SOM-a\_31\_n125\_m12.txt & 4 & 7.793578 & TRUE & 2 & 5.082999945 & 50.00\% & 34.78\% \\ \hline
        SOM-a\_41\_n150\_m15.txt & 3 & 17.9386929 & TRUE & 1 & 6.620999813 & 66.67\% & 63.09\% \\ \hline
        SOM-b\_13\_n400\_m40.txt & 0 & 149.5033364 & FALSE & 0 & 36.16199994 & \#DIV/0! & 75.81\% \\ \hline
        SOM-b\_17\_n500\_m50.txt & 0 & 175.8397833 & FALSE & 0 & 59.06400013 & \#DIV/0! & 66.41\% \\ \hline
        SOM-b\_1\_n100\_m10.txt & 4 & 3.5477426 & TRUE & 2 & 3.808000088 & 50.00\% & -7.34\% \\ \hline
        SOM-b\_7\_n200\_m60.txt & 0 & 122.7233354 & FALSE & 0 & 56.852 & \#DIV/0! & 53.67\% \\ \hline
        SOM-b\_9\_n300\_m30.txt & 0 & 130.224927 & FALSE & 0 & 20.5619998 & \#DIV/0! & 84.21\% \\ \hline
    \end{tabular}
\end{table}
\end{landscape}



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections


%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-num-names} 
\bibliography{cas-refs}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{00}

% %% \bibitem[Author(year)]{label}
% %% Text of bibliographic item

% \bibitem[ ()]{}

% \end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
